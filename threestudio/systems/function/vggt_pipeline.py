import numpy as np
from vggt.models.vggt import VGGT
from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map

from dataclasses import dataclass
import torch
from PIL import Image
from torchvision import transforms as TF
from scipy.spatial import cKDTree

from .fusion_point import fusion_point_clouds


@dataclass
class VGGTPipelineConfig:
    """VGGT ç‚¹äº‘ç”Ÿæˆå‚æ•°"""
    model_path: str = "facebook/VGGT-1B"
    device: str = "cuda"
    dtype: str = "float16"  # Changed to float16 to match model weights
    conf_thres: float = 50.0  # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç‚¹
    mask_black_bg: bool = False  # æ˜¯å¦å»é™¤é»‘è‰²èƒŒæ™¯
    mask_white_bg: bool = False  # æ˜¯å¦å»é™¤ç™½è‰²èƒŒæ™¯
    prediction_mode: str = (
        "Predicted Pointmap"  # æ¨¡å¼é€‰æ‹© ("Predicted Pointmap" / "Depthmap")
    )
    multi_reference: bool = True  # æ˜¯å¦ä½¿ç”¨å¤šå‚è€ƒå¸§



class VGGTPipeline:
    def __init__(self, cfg: VGGTPipelineConfig):
        self.cfg = cfg
        # Convert string dtype to torch.dtype
        self.dtype = getattr(torch, cfg.dtype) if hasattr(torch, cfg.dtype) else torch.float16
        self.model = VGGT.from_pretrained(cfg.model_path).to(cfg.device)


        #ç”Ÿæˆ extrinsics
        self.extrinsics = self.create_extrinsics_from_angles([0, 45, 90, 180, 270, 315], radius=1.0, device="cuda")


    def __del__(self):
        del self.model
        torch.cuda.empty_cache()

    def preprocess_images(self, images: np.ndarray = None, mode="crop"):
        cfg = self.cfg
        # Check for empty list
        if len(images) == 0:
            raise ValueError("At least 1 image is required")
        # Validate mode
        if mode not in ["crop", "pad"]:
            raise ValueError("Mode must be either 'crop' or 'pad'")

        result_images = []
        shapes = set()
        to_tensor = TF.ToTensor()
        target_size = 518

        # First process all images and collect their shapes
        for img in images:
            width, height = img.size
            if mode == "pad":
                # Make the largest dimension 518px while maintaining aspect ratio
                if width >= height:
                    new_width = target_size
                    new_height = (
                        round(height * (new_width / width) / 14) * 14
                    )  # Make divisible by 14
                else:
                    new_height = target_size
                    new_width = (
                        round(width * (new_height / height) / 14) * 14
                    )  # Make divisible by 14
            else:  # mode == "crop"
                # Original behavior: set width to 518px
                new_width = target_size
                # Calculate height maintaining aspect ratio, divisible by 14
                new_height = round(height * (new_width / width) / 14) * 14

            # Resize with new dimensions (width, height)
            img = img.resize((new_width, new_height), Image.Resampling.BICUBIC)
            img = to_tensor(img)  # Convert to tensor (0, 1)

            # Center crop height if it's larger than 518 (only in crop mode)
            if mode == "crop" and new_height > target_size:
                start_y = (new_height - target_size) // 2
                img = img[:, start_y : start_y + target_size, :]

            # For pad mode, pad to make a square of target_size x target_size
            if mode == "pad":
                h_padding = target_size - img.shape[1]
                w_padding = target_size - img.shape[2]

                if h_padding > 0 or w_padding > 0:
                    pad_top = h_padding // 2
                    pad_bottom = h_padding - pad_top
                    pad_left = w_padding // 2
                    pad_right = w_padding - pad_left

                    # Pad with white (value=1.0)
                    img = torch.nn.functional.pad(
                        img,
                        (pad_left, pad_right, pad_top, pad_bottom),
                        mode="constant",
                        value=1.0,
                    )

            shapes.add((img.shape[1], img.shape[2]))
            result_images.append(img)

        # Check if we have different shapes
        # In theory our model can also work well with different shapes
        if len(shapes) > 1:
            print(f"Warning: Found images with different shapes: {shapes}")
            # Find maximum dimensions
            max_height = max(shape[0] for shape in shapes)
            max_width = max(shape[1] for shape in shapes)

            # Pad images if necessary
            padded_images = []
            for img in result_images:
                h_padding = max_height - img.shape[1]
                w_padding = max_width - img.shape[2]

                if h_padding > 0 or w_padding > 0:
                    pad_top = h_padding // 2
                    pad_bottom = h_padding - pad_top
                    pad_left = w_padding // 2
                    pad_right = w_padding - pad_left

                    img = torch.nn.functional.pad(
                        img,
                        (pad_left, pad_right, pad_top, pad_bottom),
                        mode="constant",
                        value=1.0,
                    )
                padded_images.append(img)
            result_images = padded_images
        result_images = torch.stack(result_images)  # concatenate images

        # Ensure correct shape when single image
        if len(images) == 1:
            # Verify shape is (1, C, H, W)
            if result_images.dim() == 3:
                result_images = result_images.unsqueeze(0)
        return result_images
    
    def create_extrinsics_from_angles(self,angles_deg, radius=1.0, device="cuda"):
        """
        æ ¹æ®ç»•Yè½´çš„è§’åº¦ç”Ÿæˆç›¸æœºå¤–å‚çŸ©é˜µ extrinsics: [1, S, 4, 4]
        angles_deg: list æˆ– numpy æ•°ç»„ï¼Œä¾‹å¦‚ [0, 45, 90, 180, 270, 315]
        radius: ç›¸æœºåˆ°åŸç‚¹çš„è·ç¦»
        """
        angles = torch.tensor(angles_deg, dtype=torch.float32, device=device)
        angles_rad = torch.deg2rad(angles)

        S = len(angles)
        extrinsics = torch.eye(4, device=device).repeat(1, S, 1, 1)  # [1,S,4,4]

        for i, theta in enumerate(angles_rad):
            # ç»• Y è½´æ—‹è½¬çŸ©é˜µï¼ˆç›¸æœºç»•ç‰©ä½“ï¼‰
            R = torch.tensor([
                [torch.cos(theta), 0, torch.sin(theta)],
                [0, 1, 0],
                [-torch.sin(theta), 0, torch.cos(theta)]
            ], device=device)

            # å¹³ç§»ï¼šç›¸æœºåœ¨åœ†å‘¨ä¸Šï¼Œé¢å‘åŸç‚¹
            t = torch.tensor([
                radius * torch.sin(theta),
                0.0,
                radius * torch.cos(theta)
            ], device=device)

            extrinsics[0, i, :3, :3] = R.T     # æ³¨æ„æ˜¯ä»ä¸–ç•Œåˆ°ç›¸æœº â†’ å–è½¬ç½®
            extrinsics[0, i, :3, 3] = -R.T @ t # ç›¸æœºä¸­å¿ƒä½ç½®è½¬ä¸ºå¤–å‚å½¢å¼

        return extrinsics  # [1,S,4,4]

    def run(self, images: np.ndarray = None):
        """
        ä¸€é”®è¿è¡Œ VGGT æ¨ç†å¹¶ç”Ÿæˆç‚¹äº‘ (coords, rgb)ã€‚

        æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
        - å•æ¬¡æ¨ç†æ¨¡å¼ï¼ˆcfg.multi_reference=Falseï¼‰ï¼šä¸€æ¬¡æ€§è¾“å…¥æ‰€æœ‰è§†è§’
        - å¤šå‚è€ƒè§†è§’æ¨¡å¼ï¼ˆcfg.multi_reference=Trueï¼‰ï¼šæ¯å¼ å›¾ç‰‡ä¾æ¬¡ä½œä¸ºå‚è€ƒè§†è§’ï¼Œç‹¬ç«‹æ¨ç†
        """
        cfg = self.cfg
        self.model.eval()
        print(f"ğŸš€ å¼€å§‹ VGGT æ¨ç†ï¼Œè¾“å…¥å›¾ç‰‡æ•°é‡: {len(images)}ï¼Œè®¾å¤‡: {cfg.device}")

        # ---- Step 1: å›¾ç‰‡é¢„å¤„ç† ----
        images_tensor = self.preprocess_images(images).to(cfg.device)
        print(f"é¢„å¤„ç†åå›¾ç‰‡å½¢çŠ¶: {images_tensor.shape} (batch, channels, height, width)")

        # åˆ¤æ–­æ¨ç†æ¨¡å¼
        if not getattr(cfg, "multi_reference", False):
            print("ğŸ§© å½“å‰ä¸ºã€å•æ¬¡æ¨ç†æ¨¡å¼ã€‘")
            all_coords, all_rgb, all_conf = self._run_single_inference(images_tensor)
        else:
            print("ğŸ” å½“å‰ä¸ºã€å¤šå‚è€ƒè§†è§’æ¨¡å¼ã€‘")
            all_coords, all_rgb, all_conf = self._run_multi_reference_inference_fused(images_tensor)

        print(f"âœ… VGGT ç‚¹äº‘ç”Ÿæˆå®Œæˆï¼Œæ€»ç‚¹æ•°: {all_coords.shape[0]}")
        torch.cuda.empty_cache()
        return all_coords, all_rgb, all_conf


    def _predictions_to_pointcloud(self, predictions):
        """æå–é¢„æµ‹ç‚¹äº‘çš„é€šç”¨å‡½æ•°"""

        cfg = self.cfg
        if not isinstance(predictions, dict):
            raise ValueError("predictions å¿…é¡»æ˜¯ä¸€ä¸ªå­—å…¸")

        if "Pointmap" in cfg.prediction_mode and "world_points" in predictions:
            world_points = predictions["world_points"]
            conf = predictions.get("world_points_conf", np.ones_like(world_points[..., 0]))
        else:
            world_points = predictions["world_points_from_depth"]
            conf = predictions.get("depth_conf", np.ones_like(world_points[..., 0]))

        images = predictions["images"]

        if images.ndim == 4 and images.shape[1] == 3:
            images = np.transpose(images, (0, 2, 3, 1))

        coords = world_points.reshape(-1, 3)
        coords[..., 1:] *= -1

        rgb = (images.reshape(-1, 3) * 255).astype(np.uint8)
        conf = conf.reshape(-1)

        conf_threshold = np.percentile(conf, cfg.conf_thres) if cfg.conf_thres > 0 else 0.0
        mask = (conf >= conf_threshold) & (conf > 1e-5)

        if cfg.mask_black_bg:
            black_mask = rgb.sum(axis=1) >= 16
            mask &= black_mask
        if cfg.mask_white_bg:
            white_mask = ~((rgb[:, 0] > 240) & (rgb[:, 1] > 240) & (rgb[:, 2] > 240))
            mask &= white_mask

        coords = coords[mask]
        rgb = rgb[mask]

        if coords.size == 0:
            coords = np.array([[0.0, 0.0, 0.0]])
            rgb = np.array([[255, 255, 255]], dtype=np.uint8)
        return coords, rgb

    def _conf_to_pointcloud_with_conf(self, predictions,use_confidence_color=False):

        """
        å°†æ¨¡å‹è¾“å‡º predictions è½¬ä¸ºç‚¹äº‘ + ç½®ä¿¡åº¦
        è¿”å›:
            coords (N,3), rgb (N,3), conf (N,)
        """
        # è¿™é‡Œå‡è®¾ Pointmap åˆ†æ”¯ï¼Œconf é»˜è®¤ä½¿ç”¨ world_points_conf
        cfg = self.cfg
        if not isinstance(predictions, dict):
            raise ValueError("predictions å¿…é¡»æ˜¯ä¸€ä¸ªå­—å…¸")

        # ---- Step 1: é€‰æ‹©ç‚¹ä¸ç½®ä¿¡åº¦æ¥æº ----
        if "Pointmap" in cfg.prediction_mode and "world_points" in predictions:
            world_points = predictions["world_points"]
            conf = predictions.get("world_points_conf", np.ones_like(world_points[..., 0]))
        else:
            world_points = predictions["world_points_from_depth"]
            conf = predictions.get("depth_conf", np.ones_like(world_points[..., 0]))

        images = predictions["images"]

        # ---- Step 2: å›¾ç‰‡ç»´åº¦æ ‡å‡†åŒ– ----
        if images.ndim == 4 and images.shape[1] == 3:
            images = np.transpose(images, (0, 2, 3, 1))

        coords = world_points.reshape(-1, 3)
        coords[..., 1:] *= -1

        # ---- Step 3: ç½®ä¿¡åº¦æ˜ å°„ä¸ºé¢œè‰² (æ›¿ä»£ RGB) ----
        conf = conf.reshape(-1)
        conf_norm = (conf - conf.min()) / (conf.max() - conf.min() + 1e-8)

        # âœ… ä½¿ç”¨ matplotlib colormap æ˜ å°„ä¸ºä¼ªå½©è‰²ï¼ˆå¦‚â€œturboâ€æˆ–â€œviridisâ€ï¼‰
        # import matplotlib.cm as cm
        # colormap = cm.get_cmap("turbo")
        # rgb = (colormap(conf_norm)[:, :3] * 255).astype(np.uint8)

        # å¦‚æœä»å¸Œæœ›ä¿ç•™åŸå§‹å›¾ç‰‡é¢œè‰²ï¼Œå¯æ·»åŠ ä¸€ä¸ªé…ç½®å¼€å…³
        if not use_confidence_color:
            rgb = (images.reshape(-1, 3) * 255).astype(np.uint8)
        else:
            import matplotlib.cm as cm
            colormap = cm.get_cmap("turbo")
            rgb = (colormap(conf_norm)[:, :3] * 255).astype(np.uint8)


        # ---- Step 4: æ©ç è¿‡æ»¤ ----
        conf_threshold = np.percentile(conf, cfg.conf_thres) if cfg.conf_thres > 0 else 0.0

        mask = (conf >= conf_threshold) & (conf > 1e-5)

        if cfg.mask_black_bg:
            black_mask = rgb.sum(axis=1) >= 16
            mask &= black_mask
        if cfg.mask_white_bg:
            white_mask = ~((rgb[:, 0] > 240) & (rgb[:, 1] > 240) & (rgb[:, 2] > 240))
            mask &= white_mask

        coords = coords[mask]
        rgb = rgb[mask]
        conf = conf_norm[mask]


        if coords.size == 0:
            coords = np.array([[0.0, 0.0, 0.0]])
            rgb = np.array([[255, 255, 255]], dtype=np.uint8)
        return coords, rgb.astype(np.uint8), conf

    def _conf_to_pointcloud(self, predictions):
        """æå–é¢„æµ‹ç‚¹äº‘çš„é€šç”¨å‡½æ•°ï¼Œå¯é€‰æ‹©ä½¿ç”¨ç½®ä¿¡åº¦ä»£æ›¿ RGB å¯è§†åŒ–"""

        cfg = self.cfg
        if not isinstance(predictions, dict):
            raise ValueError("predictions å¿…é¡»æ˜¯ä¸€ä¸ªå­—å…¸")

        # ---- Step 1: é€‰æ‹©ç‚¹ä¸ç½®ä¿¡åº¦æ¥æº ----
        if "Pointmap" in cfg.prediction_mode and "world_points" in predictions:
            world_points = predictions["world_points"]
            conf = predictions.get("world_points_conf", np.ones_like(world_points[..., 0]))
            print(f"world_points shape: {world_points.shape}")
            print(f"conf shape: {conf.shape}")

        else:
            world_points = predictions["world_points_from_depth"]
            conf = predictions.get("depth_conf", np.ones_like(world_points[..., 0]))

        images = predictions["images"]

        # ---- Step 2: å›¾ç‰‡ç»´åº¦æ ‡å‡†åŒ– ----
        if images.ndim == 4 and images.shape[1] == 3:
            images = np.transpose(images, (0, 2, 3, 1))

        coords = world_points.reshape(-1, 3)
        coords[..., 1:] *= -1

        # ---- Step 3: ç½®ä¿¡åº¦æ˜ å°„ä¸ºé¢œè‰² (æ›¿ä»£ RGB) ----
        conf = conf.reshape(-1)
        conf_norm = (conf - conf.min()) / (conf.max() - conf.min() + 1e-8)

        # âœ… ä½¿ç”¨ matplotlib colormap æ˜ å°„ä¸ºä¼ªå½©è‰²ï¼ˆå¦‚â€œturboâ€æˆ–â€œviridisâ€ï¼‰
        import matplotlib.cm as cm
        colormap = cm.get_cmap("turbo")
        rgb = (colormap(conf_norm)[:, :3] * 255).astype(np.uint8)

        # å¦‚æœä»å¸Œæœ›ä¿ç•™åŸå§‹å›¾ç‰‡é¢œè‰²ï¼Œå¯æ·»åŠ ä¸€ä¸ªé…ç½®å¼€å…³
        # if not cfg.use_confidence_color:
        #     rgb = (images.reshape(-1, 3) * 255).astype(np.uint8)

        # ---- Step 4: æ©ç è¿‡æ»¤ ----
        conf_threshold = np.percentile(conf, cfg.conf_thres) if cfg.conf_thres > 0 else 0.0

        mask = (conf >= conf_threshold) & (conf > 1e-5)

        if cfg.mask_black_bg:
            black_mask = rgb.sum(axis=1) >= 16
            mask &= black_mask
        if cfg.mask_white_bg:
            white_mask = ~((rgb[:, 0] > 240) & (rgb[:, 1] > 240) & (rgb[:, 2] > 240))
            mask &= white_mask

        coords = coords[mask]
        rgb = rgb[mask]

        if coords.size == 0:
            coords = np.array([[0.0, 0.0, 0.0]])
            rgb = np.array([[255, 255, 255]], dtype=np.uint8)

        return coords, rgb

    def _run_single_inference(self, images_tensor: torch.Tensor):
        cfg = self.cfg
        with torch.no_grad():
            with torch.cuda.amp.autocast(enabled=False):
                predictions = self.model(images_tensor,camera_extrinsics=self.extrinsics)


        extrinsic, intrinsic = pose_encoding_to_extri_intri(
            predictions["pose_enc"], images_tensor.shape[-2:]
        )
        predictions["extrinsic"] = extrinsic
        predictions["intrinsic"] = intrinsic

        # è½¬ numpy
        for key in predictions.keys():
            if isinstance(predictions[key], torch.Tensor):
                predictions[key] = predictions[key].cpu().numpy().squeeze(0)

        # æ·±åº¦å›¾ â†’ ç‚¹äº‘
        depth_map = predictions["depth"]
        world_points = unproject_depth_map_to_point_map(
            depth_map, predictions["extrinsic"], predictions["intrinsic"]
        )
        predictions["world_points_from_depth"] = world_points

        # è½¬æ¢ä¸ºç‚¹äº‘
        coords, rgb, conf = self._conf_to_pointcloud_with_conf(predictions)
        # å°†confè®¾ç½®ä¸ºå…¨1
        conf = np.ones_like(conf)
        return coords, rgb, conf

    def _run_multi_reference_inference_fused(self, images_tensor: torch.Tensor):
        """
        å¤šå‚è€ƒå›¾æ¨ç† + ç½®ä¿¡åº¦åŠ æƒèåˆç‚¹äº‘ï¼ˆç»Ÿä¸€åˆ°ä¸–ç•Œåæ ‡ç³»ï¼Œé€‚é…3Ã—4å¤–å‚ï¼‰
        Args:
            images_tensor (torch.Tensor): å¤šè§†è§’å›¾ç‰‡ (N, 3, H, W)ï¼ŒN=6
        Returns:
            coords_fused (np.ndarray): èåˆåçš„ç‚¹äº‘åæ ‡ (M, 3)
            rgb_fused (np.ndarray): èåˆåçš„ç‚¹äº‘é¢œè‰² (M, 3)
        """
        cfg = self.cfg
        num_views = images_tensor.shape[0]  # num_views=6
        all_coords_world = []  # å­˜å‚¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‚¹äº‘
        all_rgb = []
        all_conf = []

        # é€‰æ‹©åŸºå‡†åæ ‡ç³»ï¼šè¿™é‡Œä»¥ç¬¬0å¼ å›¾çš„ç›¸æœºåæ ‡ç³»ä¸ºç»Ÿä¸€åŸºå‡†
        base_view_idx = 0


        for ref_idx in range(num_views):
            print(f"\nğŸ“¸ ä»¥ç¬¬ {ref_idx} å¼ å›¾ç‰‡ä½œä¸ºå‚è€ƒå›¾è¿›è¡Œæ¨ç†...")

            # é‡æ’å›¾ç‰‡ï¼šå‚è€ƒå›¾æ”¾ç¬¬ä¸€ä½ï¼Œå…¶ä½™é¡ºåºä¸å˜
            reordered = torch.cat([
                images_tensor[ref_idx:ref_idx+1],
                torch.cat([images_tensor[:ref_idx], images_tensor[ref_idx+1:]], dim=0)
            ], dim=0).unsqueeze(0)  # è¾“å‡ºå½¢çŠ¶: (1, 6, 3, H, W)

            # æ¨¡å‹æ¨ç†ï¼ˆç¦ç”¨æ¢¯åº¦å’Œæ··åˆç²¾åº¦ï¼‰
            with torch.no_grad():
                with torch.cuda.amp.autocast(enabled=False):
                    # predictions: dict, keys: ['pose_enc', 'depth', 'world_points', 'world_points_conf']
                    # world_points: (6, 518,518, 3)

                    predictions = self.model(reordered)

            # è§£æç›¸æœºå‚æ•°ï¼ˆå¤–å‚3Ã—4ï¼Œå†…å‚3Ã—3ï¼‰
            extrinsic, intrinsic = pose_encoding_to_extri_intri(
                predictions["pose_enc"], reordered.shape[-2:]
            )  # extrinsic: (1,6,3,4), intrinsic: (1,6,3,3)
            predictions["extrinsic"] = extrinsic
            predictions["intrinsic"] = intrinsic
            # Tensorè½¬numpyï¼Œå»é™¤batchç»´åº¦
            for key in predictions.keys():
                if isinstance(predictions[key], torch.Tensor):
                    predictions[key] = predictions[key].cpu().numpy().squeeze(0)  # å¤–å‚å˜ä¸º(6,3,4)ï¼Œå†…å‚å˜ä¸º(6,3,3)

            # --------------------------
            # å…³é”®ï¼šç»Ÿä¸€world_pointsåˆ°åŸºå‡†åæ ‡ç³»
            # --------------------------
            # 1. è·å–å½“å‰æ¨ç†ä¸­å„è§†è§’çš„å¤–å‚ï¼ˆé‡æ’åçš„é¡ºåºï¼‰
            #    å¤–å‚extrinsicæ ¼å¼ï¼š[R|t]ï¼Œè¡¨ç¤ºç›¸æœºåæ ‡ç³»â†’ä¸–ç•Œåæ ‡ç³»çš„è½¬æ¢ï¼ˆä¸–ç•Œåæ ‡ç³»ä»¥ref_idxä¸ºåŸºå‡†ï¼‰
            #    å³ï¼šP_world = R * P_cam + t
            current_extrinsics = predictions["extrinsic"]  # shape: (6, 3, 4)

            # 2. é‡æ’å¤–å‚å›åŸå§‹é¡ºåºï¼ˆä¸åŸå§‹images_tensorçš„è§†è§’å¯¹åº”ï¼‰
            reorder_indices = [ref_idx] + [i for i in range(num_views) if i != ref_idx]
            inverse_indices = np.argsort(reorder_indices)  # åŸå§‹é¡ºåºç´¢å¼•
            extrinsics_original_order = current_extrinsics[inverse_indices]  # æ¢å¤ä¸ºåŸå§‹è§†è§’é¡ºåºçš„å¤–å‚

            # 3. è®¡ç®—ä»â€œå½“å‰å‚è€ƒå›¾åæ ‡ç³»â€åˆ°â€œåŸºå‡†åæ ‡ç³»â€çš„è½¬æ¢çŸ©é˜µ
            #    åŸºå‡†åæ ‡ç³»æ˜¯base_view_idxçš„ç›¸æœºåæ ‡ç³»ï¼Œå½“å‰ä¸–ç•Œåæ ‡ç³»æ˜¯ref_idxçš„ç›¸æœºåæ ‡ç³»
            #    è½¬æ¢å…³ç³»ï¼šP_base = T * P_current_world
            R_ref, t_ref = extrinsics_original_order[ref_idx, :3, :3], extrinsics_original_order[ref_idx, :3, 3:]  # ref_idxåœ¨åŸå§‹é¡ºåºä¸­çš„å¤–å‚
            R_base, t_base = extrinsics_original_order[base_view_idx, :3, :3], extrinsics_original_order[base_view_idx, :3, 3:]  # åŸºå‡†è§†è§’çš„å¤–å‚

            # å¤–å‚çš„é€†ï¼šä¸–ç•Œåæ ‡ç³»ï¼ˆref_idxï¼‰â†’ ç›¸æœºåæ ‡ç³»ï¼ˆref_idxï¼‰
            R_ref_inv = R_ref.T  # æ—‹è½¬çŸ©é˜µçš„é€†=è½¬ç½®ï¼ˆæ­£äº¤çŸ©é˜µï¼‰
            t_ref_inv = -R_ref_inv @ t_ref  # å¹³ç§»çš„é€†

            # ä»ref_idxä¸–ç•Œåæ ‡ç³» â†’ base_view_idxç›¸æœºåæ ‡ç³»çš„è½¬æ¢çŸ©é˜µï¼ˆ4x4é½æ¬¡çŸ©é˜µï¼‰
            T = np.eye(4)
            T[:3, :3] = R_base @ R_ref_inv  # æ—‹è½¬éƒ¨åˆ†ï¼šrefç›¸æœºåæ ‡ç³» â†’ baseç›¸æœºåæ ‡ç³»
            T[:3, 3:] = R_base @ t_ref_inv + t_base  # å¹³ç§»éƒ¨åˆ†

            # 4. è½¬æ¢å½“å‰æ¨ç†çš„world_pointsåˆ°åŸºå‡†åæ ‡ç³»
            world_points = predictions["world_points"]  # shape: (6, 518, 518, 3)ï¼ˆ6ä¸ªè§†è§’çš„æ·±åº¦å›¾ç”Ÿæˆçš„åæ ‡ï¼‰
            H, W = world_points.shape[1], world_points.shape[2]
            num_points_per_view = H * W  # æ¯ä¸ªè§†è§’çš„ç‚¹æ•°ï¼š518*518

            # å±•å¹³ä¸º(N_views, N_points, 3)ï¼Œæ–¹ä¾¿æ‰¹é‡è½¬æ¢
            world_points_flat = world_points.reshape(num_views, num_points_per_view, 3)  # (6, 518*518, 3)

            # é½æ¬¡åæ ‡è½¬æ¢ï¼ˆæ·»åŠ w=1ï¼‰
            world_points_hom = np.concatenate([
                world_points_flat,
                np.ones((num_views, num_points_per_view, 1), dtype=np.float32)
            ], axis=-1)  # (6, 518*518, 4)

            # åº”ç”¨è½¬æ¢çŸ©é˜µTï¼šP_unified = T @ P_current_world
            world_points_unified_hom = (T @ world_points_hom.transpose(0, 2, 1)).transpose(0, 2, 1)  # (6, 518*518, 4)
            world_points_unified = world_points_unified_hom[..., :3]  # å»é™¤é½æ¬¡åˆ†é‡ï¼Œ(6, 518*518, 3)

            # æ¢å¤åŸå§‹å½¢çŠ¶(6, 518, 518, 3)å¹¶å­˜å‚¨
            world_points_unified_reshaped = world_points_unified.reshape(num_views, H, W, 3)
            predictions["world_points"] = world_points_unified_reshaped


            # æå–ç‚¹äº‘ã€é¢œè‰²ã€ç½®ä¿¡åº¦ï¼ˆç›¸æœºåæ ‡ç³»ä¸‹ï¼‰
            coords_cam, rgb, conf = self._conf_to_pointcloud_with_conf(predictions)  # (M,3), (M,3), (M,)


            x_mean = np.mean(coords_cam[:, 0])
            y_mean = np.mean(coords_cam[:, 1])
            z_mean = np.mean(coords_cam[:, 2])
            coords_cam -= np.array([x_mean, y_mean, z_mean])

            # ç”¨numpyä¿å­˜coords_cam, rgb, confæ•°æ®
            # np.savez_compressed(
            #     f"init_3dgs_{ref_idx}.npz",
            #     coords_cam=coords_cam,
            #     rgb=rgb,
            #     conf=conf
            # )


            # å­˜å‚¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ç‚¹äº‘æ•°æ®
            all_coords_world.append(coords_cam)
            all_rgb.append(rgb)
            all_conf.append(conf)

            print(f"âœ… ç¬¬ {ref_idx} å¼ å‚è€ƒå›¾å®Œæˆï¼š{coords_cam.shape[0]} ä¸ªç‚¹ï¼ˆå·²è½¬ä¸–ç•Œåæ ‡ç³»ï¼‰")
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
            del predictions  # é‡Šæ”¾å†…å­˜
        print(len(all_coords_world))

        coords_fused, rgb_fused, conf_fused = fusion_point_clouds(
            all_coords_world,
            all_rgb,
            all_conf,
        )


        print(f"ğŸ¯ å¤šå‚è€ƒå›¾èåˆå®Œæˆï¼šæ€»ç‚¹æ•° {coords_fused.shape[0]}")
        return coords_fused, rgb_fused, conf_fused